# Day 1: RAG
- depending on knowledge-base (a database) to determine the context of a model

# Day 2: LangChain for loading knowledge-base

# Day 3: Mastering Vector Embeddings OpenAI

# Day 4: Mastering Retrieval-Augmented Generation Hands-On LLM Integration
cleaning memories by:
        # set up a new conversation memory for the chat
        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

        # putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
        conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)